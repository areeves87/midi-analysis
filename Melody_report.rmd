---
title: "Melody Track Identification"
author: "Alex Reeves"
date: "March 3, 2018"
output: html_document
---

```{r include=FALSE, opts_chunk$set(echo=FALSE)}
library(tidyverse)
library(randomForest)
library(party)
library(pROC)
library(ROCR)
library(PRROC)
library(knitr)
library(kableExtra)
library(memisc)
```

```{r include=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
        library(grid)
        
        # Make a list from the ... arguments and plotlist
        plots <- c(list(...), plotlist)
        
        numPlots = length(plots)
        
        # If layout is NULL, then use 'cols' to determine layout
        if (is.null(layout)) {
                # Make the panel
                # ncol: Number of columns of plots
                # nrow: Number of rows needed, calculated from # of cols
                layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                                 ncol = cols, nrow = ceiling(numPlots/cols))
        }
        
        if (numPlots==1) {
                print(plots[[1]])
                
        } else {
                # Set up the page
                grid.newpage()
                pushViewport(viewport(layout = grid.layout(nrow(layout), 
                                                           ncol(layout)
                                                           )
                                      )
                             )
                
                # Make each plot, in the correct location
                for (i in 1:numPlots) {
        # Get the i,j matrix positions of the regions that contain this subplot
                        matchidx <- as.data.frame(which(layout == i, 
                                                        arr.ind = TRUE)
                                                  )
                        
                        print(plots[[i]], 
                              vp = viewport(layout.pos.row = matchidx$row,
                                            layout.pos.col = matchidx$col)
                              )
                }
        }
}
```

#Abstract

MIDI files have tracks containing musical note pitch and timing information. Usually, one of those tracks is the melody and the others are accompaniment. The goal of the work is to automatically identify the melody in 226 hand-labeled Final Fantasy MIDI transcriptions using the statistical properties of the music tracks. Finding the melody automatically could be useful for a number of tasks, like speeding up melody matching when searching MIDI databases or motif extraction. Here I show a method for automatically classifying a MIDI track as melody or non-melody with an AUC-ROC of 0.926 and an AUC-PR of 0.772 using a conditional random forest model. Given a set of predictors calculated from a MIDI track, the cRF model outputs a probability that the track contains a melody.

*Name the most important predictors?*

#Introduction

There are tens of thousands of videogame music transcriptions in MIDI format that can be found for free online. The transcriptions are typically the result of musically-literate fans of the game listening to and writing out all the parts of a song within a digital audio workstation (such as FLStudio, Reaper, Garageband, etc) and then converting the file to MIDI format. Sometimes as part of a composer's work portfolio, other times it is done by amateur enthusiasts. The transcriptions are often hosted in a website after they have been vetted by the website's administrators or designated website staff. Thus the transcriptions are not only plentiful but also are carefully curated and can be relied on as accurate symbolic representations of the original work. 

Most of these MIDIs contain a number of tracks, one of which is the melody and the others are accompaniement. The goal of this work is to automatically identify the melody track from the stastical properties of the MIDI file. This methodology could be extended to other symbolic music file types such as XML or digital score since it relies on note pitch and duration information, which is essential to any symbolic representation of music. The main difficulty would be in writing the file parser so that track information from the new file type could be read into the routine. 

Although I started this project without serious consideration of the practical benefits of automatically selecting the melody track, in hindsight there are good applications. For example, if you wanted to query a database for a melody, it would reduce the search space if you knew or could accurately predict which track would contain the melody. This is because most of the time what people get stuck in their head is the melody, so their query is usually a melody. Another application would be extracting themes and motifs from corpuses, since melodies typically contain the themes and motifs.

This work follows the approach of David Rizo's paper. He implimeneted machine learning instead of simple heuristics based on note statistics or track names. Since his paper, there have been many interesting developments in the field of music information retrieval. 

Since his paper, others have implimeneted his approach while also bearing in mind the imbalanced nature of melody selection. That is, accompaniement tracks typically outnumber melody tracks 4:1. They improve their algorithm's performance by upsampling melodies or downsampling accompaniment.

To date, the work of identifying and parsing melodies from MIDI files mostly focuses on classical, jazz, and pop music genres. I try to extend this approach to videogame music, specifically the Japenese Role Playing Game series Final Fantasy, a musical collection born and bred in the MIDI format. The genre borrows heavily from the neo-romantic style and progressive rock from the 1970s and 1980s, although there are also many other influences. 

The dataset consists of 227 MIDI transcriptions from the game series Final Fantasy. The purpose of this project is to predict a melody track in a transcription using a supervised learning approach. This requires labeled data, which I generated by manually tagging the melody track in all 227 transcriptions. I operated under the assumption that there would usually be a single melody track but that there would probably be a few exceptions to that general rule. Using these labels I trained the classifier with one half of the data and tested the classifier using the other half. 

#Methods

I used two different programs to read in the midi files, depending on my purpose. When I wanted to find and label the melody track in a midi file, I loaded the midi file into the digital audio workstation 'Reaper'. The DAW environment made it easy to listen to the MIDI file -- a key step for having humans select a melody track. Reaper also made it easy to mute and solo specific tracks in order to verify that a particular track carried the melody.  I found the melody track by listening for the most salient and singable track, and recorded its title, track and channel in an excel spreadsheet. Any track that was not labeled a melody was by defualt tagged as a non-melody.

When I wanted to generate a table of midi data representing all the note events in a transcription, I used the R package 'TuneR' developed by Uwe Ligges et al. The package provides functions for parsing the midi file into tabular data and for coercing the tables into a tidy format consisting of rows of note events and their associated parameters such as pitch and timing. 

Using the tidy tables of note events, I summarized the music statistics of every track in the midi file corpus. Then I split the data into test and train groups in order to train and validate a classifier. The classifier learned to identify melody tracks within the training data by using the track statistics to predict the target label. Then I validated the classifier's performance using the test data.

My approach introduces a few sources of bias worth mentioning:

* I hand-labeled the data, so the labels reflect my personal intuition about what constitutes a melody. Similarly, I later engineered features based on my intuitions about the statistical properties of a melody. Thus my project goal could be restated in more cynical terms as "methods for predicting whether I would consider a track as carrying the melody." However, I have extensive musical training, which means what I consider a melody is probably what another trained musician would consider a melody.

* The MIDI files are transcriptions of the original music. Assuming they were transcribed by a human, they reflect the decisions of the transcribers about how much detail to include and how closely to adhere to the original format. In some cases, the transcription are note-perfect recreation of the orginal. In other cases, there are omission or comission errors, but none too egregious as to render the transcription unrecognizable. So an even more cynical restatemet of the project goal would be to "predict whether I think someone has transcribed the melody of a given song." On the other hand, there is a consensus implied when I select a track from their transcription as the melody: the track was salient enough for the transcriber to include it and salient enough for me to select it. 

* The MIDI files reflect the music of a single composer (Nobuo Uematsu) working within a single franchise (Final Fantasy I - VII). The degree to which a classifier trained on a composer-specific dataset is able to generalize to other composers and franchises is unclear. This the downside to the dataset since one of the most difficult pieces of data to acquire was the melody labels. 

##Midi Track Characterization

The simplified tabular data has the following format:

```{r echo=FALSE, results='asis', cache=TRUE}
data <- read.table(unz("data.zip","data/midi_db.csv"),
                   header=T,
                   quote="\"",
                   sep=",")

data[1:5,] %>%
kable("html")%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

Each row is a note event. The columns indicate that the note event is ocurring within a given midi file, at a given time (in ticks), for a given length, within the given track, from the indicated channel, at the indicated midi note number (pitch) or equivalent notename, and with the indicated velocity. 

I summarize these note events by track to get a statistical description to feed into the classifier. Below are the first few rows and a sample of columns from the summary dataset. Each row is a track within a midi file and the columns give important statistical information.

```{r echo=FALSE, results='asis', cache=TRUE}
agg <- read.table(unz("data.zip","data/midi_melodies.csv"),
                   header=T,
                   quote="\"",
                   sep=",")

agg[1:6,c(1:4,8,12)] %>%
kable("html") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

The statistical summaries of the musical data are defined by melodic and rhythmic properties as well as song-level properties. The summaries are defined in the following data dictionary. The left column indicates a summary variable and the right column gives its definition.

```{r echo=FALSE}
var_name <- names(agg)
var_descr <- c("title of the midi transcription",
               "channel # the note events stream from",
               "track # the note events are contained within",
               "target label; indicates whether or not the track is a melody",
               "number of the instrument program",
               "family of the instrument program",
               "name of the instrument program",
               "number of note events within track",
               "fraction of song the track's note lengths occupy",
               "total number tracks",
               "total number channels",
               "fraction of track notes that are song's highest",
               "pitch class entropy",
               "interval entropy",
               "inter-onset interval entropy",
               "previous note duration was shorter than current note duration",
               "median midi note value for the track",
               "median absolute deviation midi note value for the track",
               "fraction of events with multiple notes for the track")

data.frame(name = var_name, description = var_descr) %>%
kable("html") %>%
kable_styling(bootstrap_options = "striped")
```

##Feature Selection

The musical and meta properties of the midi file can be divided into song-level and track-level features. Song-level features describe the song as a whole whereas track-level features only summarize the musical data in a given track. Song-level and track-level features contributed to all nineteen descriptors in different mixtures. Three features (title, channels, tracks) exclusively used song-level information and thirteen features exclusively used track-level information. Two remaining descriptors, track_occ and top_rate, incorporated track-level information that was normalize to the properties of the song as a whole. 

The last descriptor, ME, is a target labelling indicating whether I thought the track was a melody. Although it is difficult to specify the exact features I take into consideration when selecting the melody, it is safe to say that the label represents a mixture of song-level and track-level properties. The goal of this project is essentially to model my melody selection process.

The graphs below give some insight into how one could possibly begin selecting the melody track using statistics. Each graph has two densities, the red densities are the non-melody tracks and the blue densities are the melody tracks. Some features show no appreciable difference between melody and non-melody tracks. But several features do, as evidenced by the markedly different shapes in melody vs. non-melody groups. 

For example, when looking at the 'top_rate' feature, melody tracks (blue) are especially dense between the values 0.75 and 1.00, whereas non-melody tracks (red) are uniformly distributed between 0.00 and 1.00. Similarly, melody tracks tend to have low 'channel' value and high 'med_note' value. These differences can be exploited by a classifier to score how melodic a track is. 

```{r echo=FALSE, cache=TRUE}
######Density multiplot

p1 <-   ggplot(agg, aes(x = channel, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        theme(legend.position = "none")

p2 <-   ggplot(agg, aes(x = track, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p3 <-   ggplot(agg, aes(x = track_occ, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p4 <-   ggplot(agg, aes(x = frac_poly, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p5 <-   ggplot(agg, aes(x = events, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        scale_x_continuous(limits = c(0, 1000)) +
        theme(legend.position = "none")

p6 <-   ggplot(agg, aes(x = mad_int, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p7 <-   ggplot(agg, aes(x = IOI_entropy, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p8 <-   ggplot(agg, aes(x = pc_entropy, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p9 <-   ggplot(agg, aes(x = int_entropy, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        theme(legend.position = "none")

p10 <-   ggplot(agg, aes(x = longpr, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p11 <-   ggplot(agg, aes(x = top_rate, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

p12 <-   ggplot(agg, aes(x = med_note, fill = as.factor(ME))) +
        geom_density(alpha = 0.5) +
        ylab(NULL) +
        theme(legend.position = "none")

layout <- matrix(1:12,3,4,byrow=TRUE)

multi.p1 <- multiplot(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, 
            p11, p12, layout=layout)
```


##Random Forest Classifier



I tested a few different classifiers. First it was a decision tree, then a random forest, then a conditional random forest. I found that the conditional random forest performed the best. 

What are conditional random forests? How does it perform generally? Is it robust to noise? How are the trees built and how are their decisions combined? 

The membership for class "melody" can be interpreted as a track's probability of containing a melodic line.


##Track Selection Procedure

MIDI files can have more than one track with a melody in it. For example, in one section a violin track might carry the melody whereas the next section has the melody in a flute track. So in the case where a classifier is trained to select a single melody track, the classifier would tend to incorrectly label melody tracks as non-melody. 

MIDI files can also have no melody track. Especially in video game music, the music could be a repetitive arpeggio -- think early versions of the Final Fantasy prelude theme -- that does not have a discernable melody.

In this method, only one track is selected as the melody. Yet the method has to be able to deal with midi files continaining more than one melody track. Therefore, given a file, all its tracks are classified and their probabilities of being a melodic voice are obtained. Next a threshold is applied. At least one track's probability of being a melody has to exceed the threshold, otherwise the method labels the MIDI file as having no melody.

If at least one track has a probability above threshold, then all the tracks exceeding the threshold are compared. The track with the highest probability of containing a melody is selected as the melody track. 

#Results

##Datasets

##Experiments

###Melody vs. non-melody classification

###Melodic track selection experiment

###Style specificity

###Traing set specificity

#Conclusions and Future Work